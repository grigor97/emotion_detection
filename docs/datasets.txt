1.RAVDESS

The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) contains 7356 files (total size: 24.8 GB). The database contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18.

Audio-only files

Audio-only files of all actors (01-24) are available as two separate zip files (~200 MB each):

Speech file (Audio_Speech_Actors_01-24.zip, 215 MB) contains 1440 files: 60 trials per actor x 24 actors = 1440.
Song file (Audio_Song_Actors_01-24.zip, 198 MB) contains 1012 files: 44 trials per actor x 23 actors = 1012.
Audio-Visual and Video-only files

Video files are provided as separate zip downloads for each actor (01-24, ~500 MB each), and are split into separate speech and song downloads:

Speech files (Video_Speech_Actor_01.zip to Video_Speech_Actor_24.zip) collectively contains 2880 files: 60 trials per actor x 2 modalities (AV, VO) x 24 actors = 2880.
Song files (Video_Song_Actor_01.zip to Video_Song_Actor_24.zip) collectively contains 2024 files: 44 trials per actor x 2 modalities (AV, VO) x 23 actors = 2024.
File Summary

In total, the RAVDESS collection includes 7356 files (2880+2024+1440+1012 files).

File naming convention

Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics:

Filename identifiers

Modality (01 = full-AV, 02 = video-only, 03 = audio-only).
Vocal channel (01 = speech, 02 = song).
Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).
Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.
Statement (01 = "Kids are talking by the door", 02 = "Dogs are sitting by the door").
Repetition (01 = 1st repetition, 02 = 2nd repetition).
Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).

Filename example: 02-01-06-01-02-01-12.mp4

Video-only (02)
Speech (01)
Fearful (06)
Normal intensity (01)
Statement "dogs" (02)
1st Repetition (01)
12th Actor (12)
Female, as the actor ID number is even.



2.The enterface’05 audiovisual emotion database
http://www.enterface.net/results/


Enterface’05 [7] database for emotion recognition from audio and video signals. The database has a total of 44 speakers, and each speaker spoke approximately 30 sentences. Three hundred best video clips from the database are selected, and 225 (75%) video clips are taken for training, and 75(25%) video clips are taken for testing.


3. Iemocap full release

website: http://sail.usc.edu/iemocap/

The recorded dialogs are either improvisations of affective scenarios, or performances of theatrical scripts.
They have been manually segmented into utterances.
Each utterance from either of the actors in the interaction has been evaluated categorically
over the set of: {angry, happy, sad, neutral, frustrated, excited, fearful, surprised, disgusted, other}
by at least three different annotators, and dimensionally over the axes of: valence (positive vs. negative);
activation (calm vs. excited); and dominance (passive vs. aggressive) by at least two different annotators.

In each recording of a session only one actor wears MoCap markers while both are being recorded by
microphones and cameras. Thus there are available MoCap data (facial expression, head and hand movement)
for one actor per recording, while there are wavefile and videos for both actors. The naming convention
regarding the data is e.g., Ses01F_impro01 while indicates Session1, where the Female actor is wearing
the markers and actors are performing improvisation 1. The release contains two formats: dialog format
which contains data from the entire dyadic interaction and the sentence format where the data per dialog
(recording) have been further segmented into utterances (see folders SessionX/dialog and SessionX/sentences
respectively). For the utterance format the naming is as follows: Ses01F_impro01_M000 indicates first session,
 Female actor is wearing markers, actors are performing improvisation 1 and this is the first utterance of
 the Male actor. The timing of the sentences in each dialog can be found in the lab files in SessionX/dialog/lab

The evaluations (emotional annotations) for each recording and each uterance are contained in folder
SessionX/dialog/Evaluation/. Each file provides the detailed evaluation reports for the categorical
evaluators (e.g., C-E1), the dimensional evaluators (e.g., A-E1), and the self-evaluatiors
(e.g., C-F1 or C-M1, A-F1 or A-M1). The utterance-level information can be found in the first
line of an utterance summary.  The first entry represents the start and end times for the utterance.
The second entry is the utterance name (e.g., Ses01_impro01_F003).  The third entry is the ground truth
(if no majority ground truth could be assigned, the ground truth is labeled xxx).
The final engry is the average dimensional evaluation (over the evaluators, except the self-evaluators).


